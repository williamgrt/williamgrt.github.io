{"pages":[{"title":"about","text":"CS 硕士在读。 主要从事 C++/Go 后端开发工作。 希望未来能够从事基础架构相关的岗位。","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"I&#x2F;O 多路复用","text":"为什么需要 I/O 多路复用 大部分应用都可以使用阻塞 I/O 模型就足够完成任务了。但是，有些应用需要满足以下的条件： 使用非阻塞的方式检查文件描述符是否可以执行 I/O 操作。 可以同时检查多个文件描述符。 我们可以使用非阻塞 I/O 或者多进程多线程的方式满足以上需求，但是又会带来新的问题： 如果文件描述符很多，非阻塞 I/O 需要不停轮询每个文件描述符，造成 CPU 的浪费。 如果每个文件描述符都创建一个新进程执行 I/O 操作，会带来开销过于昂贵的问题，包括创建进程、维护进程、父子进程间通信。 多线程的方法虽然会占用较少的资源，但是正确地编写线程间通信代码是一项非常复杂的工作。 我们可以使用 I/O 多路复用技术解决上面提到的问题。 什么是 I/O 多路复用I/O 多路复用允许同时监听多个文件描述符，找出任意一个文件描述符是否可以执行 I/O 操作。 I/O 多路本质上是一种同步操作。因为真正的 I/O 操作仍然是阻塞的。 I/O 多路复用在网络编程中有以下的应用场景： 客户处理多个描述符（通常是网络套接字和交互式输入）； 客户同时处理多个套接字； TCP 服务器既要处理监听套接字，又要处理已连接套接字； 服务器需要同时处理不同协议的套接字（比如同时处理 TCP 连接和 UDP 连接）。 APIs: select/poll/epollLinux 环境下提供了三组 API 用于使用多路复用技术，分别是 select、poll、epoll。 select1int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 每个参数有以下的含义： readfds、writefds、exceptfds 都是指向文件集合的指针； nfds 必须设置为三个描述符集合中的最大值加 1； timeout 设置 select 的阻塞行为，如果指定为 NULL，select 会一直阻塞。 成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。 select 存在以下的缺陷： 每次调用，程序需要拷贝一份包含所有指定的文件描述符的数据结构到内核中。当检查大量文件描述符时，拷贝操作将会占用大量的 CPU 时间。 每次调用，内核必须检查所有的文件描述符，是否处于就绪状态。如果文件描述符过多，该操作会消耗大量时间。 程序必须检查返回的数据结构中的每个文件描述符，如果文件描述符过多，这个操作会耗费大量的时间。 select 使用的数据结构 fd_set 对于被检查的文件描述符有一个上限（FD_SETSIZE），在 Linux 下的默认值是 1024。 poll1int poll (struct pollfd *fds, unsigned int nfds, int timeout); poll 的功能和 select 类似，不过 poll并没有设置被检查文件描述符的最大值。 epollepoll 是 Linux 2.6 版本中提供的新的 API，是 select 和 poll 的升级版。epoll 使用 一个文件描述符同时监听多个描述符。每当注册新的描述符时，epoll 都会把它拷贝到一个内核数据结构中，可以避免每次监听时都要把所有监听文件描述符拷贝到内核中的时间开销。 epoll 提供了三个方法：epoll_create、epoll_ctl、epoll_wait。 epoll_create 创建一个 epoll 句柄。size 表示 epoll 支持的最大描述符个数，但是这个参数在某些 Linux 实现中已经没有意义。 1int epoll_create(int size); epoll_ctl 执行对 epoll 文件描述符集合的操作。 1int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 每个参数有以下的含义： epfd：创建 epoll 对象时分配的文件描述符； op：对文件描述符集合执行的操作。epoll 定义了以下三种操作： EPOLL_CTL_ADD：添加文件描述符； EPOLL_CTL_MID：修改文件描述符监听的事件； EPOLL_CTL_DEL：删除文件描述符。 fd：需要监听的文件描述符； event：需要监听的事件。 epoll_wait 等待时间的产生，并返回所有已就绪时间的信息。 1int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 和 select/poll 不同，epoll 采用类似于事件回调的方式获取已经发生的事件信息。当注册一个文件描述符（调用 epoll_ctl 时），内核将相应的设备和文件描述符建立回调关系，一旦事件就绪，内核就会调用相应的回调方法，epoll_wait 就可以收到通知，然后处理所有已经就绪的时间。 对比epoll 的优势 没有监听文件描述符数量的限制； 只有当注册需要监听的文件描述符时，才会把 fd 拷贝到内核中。在调用 epoll_wait 时不会重复拷贝 fd。 采用类似于事件回调的机制，不需要内核依次检查每个文件描述符相应事件是否就绪。当一个事件就绪时，注册的回调函数会将事件添加到就绪列表中，内核不需要一个一个检查文件描述符上的时间是否就绪。因此，epoll 性能不会随着监听 fd 数量的上升而下降。 使用场景select 的 timeout 参数为微秒，而 poll 和 epoll 为毫秒，因此 select 更加适用于实时性要求比较高的场景。同时，select 可移植性强，如果有跨平台的需求可以考虑 select。 poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。 epoll 用于只在 Linux 上运行，并且并发连接数很高但是活跃连接比例不高的场景，因此更适用于长连接场景。如果连接数量不多，拷贝的文件描述符不多，不能体现出 epoll 的优势。如果活跃连接数多，变化频繁，并且连接都是短暂的，也不适用于 epoll。因为 epoll 中的所有描述符都存储在内核中，每次需要对描述符的状态改变都需要通过 epoll_ctl 进行系统调用，频繁系统调用降低效率。 参考 《Linux/Unix 系统编程手册》 select/poll/epoll 对比分析 有关 I/O 多路复用的总结","link":"/2020/06/26/IO-multiplexing/"},{"title":"基于 Raft 实现容错 kv 服务（Lab 3）","text":"设计要求 整个任务要求基于 Raft 构建一个可以容错的 Key/Value 存储服务。细分为如下两个具体的任务： Part A：Key/value service without log compaction Part B：Key/value service with log compaction Part A交互流程 整体的交互流程如下： Client 找到集群中的 Leader，发送操作命令 Put/Get/Append 给 Leader，并等待 Leader 返回执行的结果。 Leader 通过底层的 Raft 协议把命令日志复制到其他的节点上。 等到集群内超过半数的节点都完成了日志的复制，Leader 执行这个命令，并且将结果返回给 Client。 在前面我们已经实现了一个可用的 Raft 协议，我们在这个任务中主要使用 Raft 提供的接口。 1func (rf *Raft) Start(command interface{}) (int, int, bool) ClientClient 的设计采用了以下的思路： 保证每个操作仅仅执行一次，因此需要一个唯一的编号表明每个客户端的操作。 保证客户端每次只发送一个请求给服务端，可以通过加锁实现。 记录当前服务端的 Leader 编号，下次访问的时候直接访问记录的 Leader，节省了寻找 Leader 的时间。 Client 的结构设计如下： 1234567type Clerk struct { servers []*labrpc.ClientEnd serial int clientId int64 leaderId int mu sync.Mutex} 当 Client 向 Server 发送 RPC 请求时，需要给每个 RPC 请求一个唯一的编号，Server 根据 Client 编号和 RPC 编号验证当前指令是否已经执行过了，保证指令的唯一执行。 ServerServer 端接收到了 Client 发送的指令后，需要通过底层的 Raft 库完成以下的事情： 主节点把命令复制到 Raft 日志中（通过 Raft 的 Start 函数），并且等待 Raft 日志已经在集群超过一半的机器上成功复制完成，并且完成最终的提交工作。 Server 接收到 Raft 库提交的日志，执行日志中的命令。 为了保证整个系统的行为是线性一致性的，Server 端需要有一个在后台运行的 apply goroutine，接收 Raft 系统提交的日志，并且执行日志的内容。所有客户端发送的命令都必须提交到 Raft 系统中，等待复制过程结束了才能执行命令的操作。 所有类型的操作都需要按照上面的过程执行，包括 Get() 操作，这种做法可以保证系统不会读到 KV 系统中的过期值。 如果 Client 发送了命令之后，Server 端的主节点变了，那么该如何处理？根据 Guide 的提示： Once the operation at that index is sent to apply(), you can tell whether or not the client’s operation succeeded based on whether the operation that came up for that index is in fact the one you put there. If it isn’t, a failure has happened and an error can be returned to the client. 我们可以通过 index 完成 Leader 变更的判断，具体的执行思路如下： 通过 Start() 的返回值，我们可以得到一个和该命令关联的 index。我们为每一个 index 创建一个 channel，用于返回 Raft 库在这个 index 上提交的命令。Client 在该 channel 上等待返回的命令，判断返回的命令和字节发送的命令是否相等。这会造成以下两个结果： 相等。说明命令执行完毕，返回 OK。 不相等。说明主节点发生了变更，Client 需要重新发送命令给新的 Leader。 Part BPart B 的任为根据 Raft 论文的内容实现基于内存的快照机制，需要阅读论文第 7 章了解整体的运行流程。 快照机制可以减少 Raft 系统日志的大小，这么做的好处有以下几点： 减少了存储 Raft 快照需要的空间。 减少日志存储的数量，加快了 Raft 节点崩溃恢复的速度。 整体流程 快照机制的运行流程如下： 复制状态机（此处是 Server）检查日志的大小，判断是否需要执行保存日志的操作。 如果需要执行保存日志的操作，Server 调用 Raft 库提供的接口保存快照，截断日志。 在系统启动的过程中，从快照恢复系统的初始状态。 快照机制还存在一个问题：如果主节点需要通过 ApplyEntries() 发送自己的日志给从节点，但此时日志已经被截断，无法发送。为了解决这个问题，Raft 作者提出了一种新的 RPC 请求：InstallSnapshot()，用于发送主节点的快照，解决了主节点发送已被截断日志的问题。 InstallSnapshotInstallSnapshot 的整体流程见下图。 在本项目的实现中，Snapshot 在一个 RPC 中全部传输完毕，不需要执行分块传输。 Raft 的 Snapshot 机制在快照大小比较小的时候，是一种比较有效的手段。但是，如果快照的大小过大（达到了 GB 级别，这种情况在实际应用中很常见），该如何处理？有以下可能的方法： 把数据存储在磁盘上。 延迟删除 log 日志的内容。这些思路可以参考作者在博士论文中的内容。 参考 extended Raft Paper Student Guide","link":"/2021/01/25/kvraft/"},{"title":"如何实现一个 Raft 协议（Lab 2 总结）","text":"最近写完了 MIT-6.824 的 Lab，收获非常多。所有 Lab 的内容加在一起是实现一个基于 Raft 的分布式可分片的容错 Key/Value Service，对于编写并发代码以及调试分布式程序有了更深刻的认识。写这几篇文章的目的是为了总结实现过程中的思路以及遇到的问题，如果能够对他人的实现有一点点启发，那就更好了 :） Raft 原理 Raft 是一种共识算法。为了实现共识，Raft 首先选举出一个 Leader，然后交由 Leader 处理日志的权限。Leader 接受客户端发送的日志，复制到其他的服务器上，并告知其他服务器什么时候可以把日志保存到状态机中。Raft 算法需要保证以下的特性： Election Safety：在一个 Term 内至多有一个 Leader。 Leader Append-Only：一个 Leader 只能添加日志，不能修改或者删除已经添加的日志。 Log Matching：如果两个日志在同一个索引位置的日志条目 Term 相等，那么两个日志在该位置以及前面的日志都是相等的。 Leader Completeness：如果一条日志在当前的 Term 内已经提交，那么在之后的 Term 中，这条日志一定会存在在 Leader 中。 State Machine Safety：如果一条日志已经 Apply 到了复制状态机的特定位置中，那么其他服务器不会在同样的位置 Apply 一条不一样的日志。（可以用于 Client 判断自己的命令是否已经被执行完毕） Raft 实现 Raft 实现网上已经有很多文章讲解了，这里也就不赘述了。实现的细节可以看一下参考资料 [2]。一些结构上的建议可以阅读参考资料 [3]，一些参考资料 [4]。正确实现 Raft 的关键就是根据论文的 Figure 2，把英语翻译成代码。如果是严格按照 Figure 2 写的，基本不会出现大问题。 这部分主要讲几个实现上的坑，或者说是注意事项。 ElectionTimerElectionTimer 的作用是让节点等待一段时间，如果接收到了主节点发送的 AppendEntires 包，就重置该定时器。否则，定时器超时后，节点转变为 Candidate 并发起选举。如果在选举的过程中又超时了，节点就需要重新发起一轮选举，直到某个节点接收到大部分节点的投票并成为新的主节点为止。整个流程如下。 在实现中，有以下几个点需要注意，同时也是我认为有可能会出错得地方。 定时器的超时时间必须是随机的。如果所有定时器的超时时间是一致的，它们会在同一时间失效，在同一时间转换为 Candidate 状态并投票给自己，这样所有的节点都不能获得多数同意并成为一个主节点，选举就会无休无止的进行。 定时器的超时时长必须要大于心跳间隔。如果定时器间隔过小，节点还没有收到 Leader 的心跳包就会超时，开始一轮新的选举。尽管所有节点已经达成了共识，但是还是会有节点发起新的选举。（血泪教训，导致后面的实现总是有问题，查代码还查不出来，明明逻辑都是没问题的。结果打印了重置定时器日志才发现超时时长设置的问题） 重置定时器的时机。论文里面提到的重置定时器只有以下几个地方：接收到了当前主节点发送的 AppendEntires 包、定时器超时自己成为 Candidate、投票给另一个 Candidate。其他时候不能重置定时器。论文中有一个地方没有写明白：当一个 Leader 收到了一个 RPC reply，并且发现了当前的 term 已经大于自己的 term，需要转变为 Follower，此时需不需要重置定时器？ AppendEntries 注意 AppendEntriesReply 的处理。Figure 2 对于 RPC request 的处理已经讲的很详细了，按照流程实现即可。但是，论文并没有对 RPC reply 的响应作出什么规定。参考资料建议的是检查当前节点的 Term 的发送请求的 Term 是否是一致的，如果不一致，说明节点在 RPC 的发送前后出现了状态变更，需要直接丢弃。并且，在此之前需要先检查 reply.Term，确保如果 Leader 已经落后则进入 Follower 状态。 不要简单截断 Follower 的 log。处理 Follower 的 log 需要严格按照论文的说明执行。这样做的原因在于：网络是不可靠的，容易出现延迟或者丢包的情况，导致先发送的 RPC 后到达 Follower，发送和接收 RPC 的顺序是不确定的。 调试 分布式系统的调试要靠日志。因为分布式系统经常会出现莫名其妙的问题，单步调试基本是行不通的，需要在系统崩溃或者测试失败后观察输出日志，反推系统出故障时的状态，检查是否存在实现上的问题。我个人推荐的做法是把每个对象（Raft 节点、RPC request、RPC response、Timer）内部状态打印出来，然后根据打印的日志推断系统变换。例如，Raft 节点的打印日志可以如下实现： 1234func (rf *Raft) String() string { return fmt.Sprintf(&quot;[%s:%d;Term:%d;VotedFor:%d;Leader:%d;LogLen:%d;CommitIndex:%d;LastApplied:%d;LastIncludedIndex:%d]&quot;, rf.state, rf.me, rf.currentTerm, rf.votedFor, rf.leaderId, len(rf.log), rf.commitIndex, rf.lastApplied, rf.lastIncludedIndex)} 这样做的好处是可以明确是哪个节点出问题了，调试时更有针对性。 同时，也可以通过在加锁 / 解锁处添加日志，判断自己的系统是否出现死锁情况（加了锁但是没有解锁）。 参考 Raft paper 助教写的 Student Guide Raft structure advice Raft locking advice","link":"/2020/11/28/raft-lab/"},{"title":"互斥锁的实现","text":"本文整理自 OSTEP 第 28 章。 在并发编程中，为了能够原子的执行一系列指令，避免单处理器的中断和多处理器的并行执行带来的问题，程序员通常使用互斥锁放在临界区周围，保证了临界区能够像单条指令一样原子执行。 自旋锁12345678910111213141516typedef struct __lock_t {int flag;} lock_t;void init(lock_t *mutex) { // 0 -&gt; lock is available, 1 -&gt; held mutex-&gt;flag = 0;}void lock(lock_t *mutex) { while (mutex-&gt;flag == 1) // TEST the flag (Line 1) ; // spin-wait (do nothing) mutex-&gt;flag = 1; // now SET it! (Line 2)}void unlock(lock_t *mutex) { mutex-&gt;flag = 0;} 这个版本的锁实现很简单。当程序加锁的时候，flag 置为 1，当程序释放锁时，flag 置为 0。当一个线程想要获得锁，但是 flag==0，那么这个线程就会不停的执行 while 循环，直到获得这个锁。这个过程称为自旋。 这个实现有一个很严重的问题，由于处理器的中断，不能保证 Line 1 和 Line 2 之间不会插入别的线程的执行。假设有两个线程 A 和 B，它们都需要访问临界区。如果线程 A 在 Line 1 执行之后被操作系统切换，线程 B 开始执行并且获得了锁，然后再次切换线程 A 执行，这个时候同时有两个线程持有一把锁，锁的正确性不能保证。 因此，锁的实现通常需要借助硬件的特殊指令，保证 Line 1 和 Line 2 之间不会插入其他线程的代码。这种指令称为原子交换（atomic exchange）。一种原子交换指令 TestAndSet 的 C 伪代码实现如下。 1234567891011121314static int flag=0;void lock(){ while(TestAndSet(&amp;flag,1)==1); // spin-wait}void unlock(){ flag=0;}int TestAndSet(int *ptr, int new) { int old = *ptr; *ptr = new; return old;} 硬件还提供了一种原子交换指令 CompareAndSwap，使用的范围更广。 1234567int CompareAndSwap(int *ptr, int expected, int new) { int actual = *ptr; if (actual == expected) { *ptr = new; } return actual;} 在处理器层面，CompareAndSwap 依赖于 cmpxchgl 指令实现。 123456789101112char compare_and_swap(int *ptr, int old, int new) { unsigned char ret; // Note that sete sets a ’byte’ not the word __asm__ __volatile__ ( &quot; lock\\n&quot; &quot; cmpxchgl %2,%1\\n&quot; &quot; sete %0\\n&quot; : &quot;=q&quot; (ret), &quot;=m&quot; (*ptr) : &quot;r&quot; (new), &quot;m&quot; (*ptr), &quot;a&quot; (old) : &quot;memory&quot;); return ret;} 上面两种自旋锁的实现不能保证等待锁的线程最终能够一定获得锁。一种解决思路是采用彩票机制，为每个等待锁的线程分配一个彩票，每个彩票对应一个值，这个值是递增的。锁的内部维护一个标识位。当标识位和线程的彩票对应的数值相等时，线程获得锁，当线程释放锁时，标识位加一。 我们可以通过硬件提供的 FetchAndAdd 指令实现。下面是这种方法的伪代码描述。 12345678910111213141516171819202122232425int FetchAndAdd(int *ptr) { int old = *ptr; *ptr = old + 1; return old;}typedef struct __lock_t { int ticket; int turn;} lock_t;void lock_init(lock_t *lock) { lock-&gt;ticket = 0; lock-&gt;turn = 0;}void lock(lock_t *lock) { int myturn = FetchAndAdd(&amp;lock-&gt;ticket); while (lock-&gt;turn != myturn) ; // spin}void unlock(lock_t *lock) { lock-&gt;turn = lock-&gt;turn + 1;} 休眠锁 自旋锁存在一个问题：当一个线程拿不到锁时，会不停的执行 while 循环，造成时间的浪费。一种改进思路是：当一个线程没有拿到锁时，直接睡眠，当锁可用时再唤醒这个线程。 我们使用 queue 作为底层数据结构，使用链表保存所有等待锁的线程。 数据结构1234567891011typedef struct __lock_t { int flag; int guard; queue_t *q;} lock_t; void lock_init(lock_t *m) { m-&gt;flag = 0; m-&gt;guard = 0; queue_init(m-&gt;q);} 其中，guard 是一个基于 TestAndSet 思路实现的自旋锁，用于访问 flag。flag 表明这个锁是否被占用。 lock1234567891011void lock(lock_t *m) { while (TestAndSet(&amp;m-&gt;guard, 1) == 1) ; if (m-&gt;flag == 0) { m-&gt;flag = 1; // lock is acquired m-&gt;guard = 0; } else { queue_add(m-&gt;q, gettid()); m-&gt;guard = 0; park(); }} 当 flag 为 0：把 flag 置为 1，表明锁被占用。当 flag 为 1 ：把线程加入到休眠队列中，等待锁的释放后被唤醒。 unlock12345678910void unlock(lock_t *m) { while (TestAndSet(&amp;m-&gt;guard, 1) == 1) ; //acquire guard lock by spinning if (queue_empty(m-&gt;q)) m-&gt;flag = 0; // let go of lock; no one wants it else unpark(queue_remove(m-&gt;q)); // hold lock // (for next thread!) m-&gt;guard = 0;} 当等待队列为空：把 flag 设置为 0，释放锁。当等待队列不为空：唤醒一个等待队列中的线程。 存在的问题 如果 queue_add() 和 park() 执行之间，有一个线程释放了锁，那么当前的线程在加入到阻塞队列后，立马就会被唤醒，这样会造成无谓的上下文切换，带来时间上的浪费。 一种解决方法是：使用一个新的系统调用 setpark()，让线程进入准备休眠的状态，如果此时有另一个线程执行了唤醒操作，当前线程立刻返回。 两阶段锁 实际情况中，锁的实现融合了两者的实现思路，Linux 中的 mutex 锁实现如下。 12345678910111213141516171819202122232425262728293031void mutex_lock (int *mutex) { int v; /* Bit 31 was clear, we got the mutex (the fastpath) */ if (atomic_bit_test_set (mutex, 31) == 0) return; atomic_increment (mutex); while (1) { if (atomic_bit_test_set (mutex, 31) == 0) { atomic_decrement (mutex); return; } /* We have to waitFirst make sure the futex value we are monitoring is truly negative (locked). */ v = *mutex; if (v&gt;= 0) continue; futex_wait (mutex, v); }}void mutex_unlock (int *mutex) { /* Adding 0x80000000 to counter results in 0 if and only if there are not other interested threads */ if (atomic_add_zero (mutex, 0x80000000)) return; /* There are other threads waiting for this mutex, wake one of them up. */ futex_wake (mutex);} 两阶段锁的设计思路如下：如果线程等待锁的过程中，另一个线程将要马上释放锁，这种情况更加适合使用自旋锁。 第一阶段会自旋若干次，试图获得锁。 如果在第一阶段没有获得锁，那么线程就会休眠，一直等待直到另一个线程唤醒它。 参考资料 Locks","link":"/2021/01/02/mutex-implementation/"},{"title":"Redis 源码阅读：启动与事件循环","text":"本文梳理了 Redis 启动服务器的流程，并且简要介绍了 Redis 事件处理机制的原理以及对应的源码实现。 Redis 作为一个高性能的缓存，必须要有一个高性能的事件处理机制支撑其性能。同时，Redis 的事件处理机制不考虑扩展性（不用于 Redis 以外的应用），保证了实现的简洁性，阅读和分析的难度不大。 本文的 Redis 版本是 redis-6.0.9，源码主要在 server.c 和 ae.c 中。 启动服务器redis-server 的 main 函数在文件 server.c 中，主要有以下几个步骤： 使用默认参数初始化。 解析命令行参数。 初始化服务器。具体执行了以下的操作： 设置信号处理函数。 初始化事件循环结构体 EventLoop。 把监听套接字作为事件注册到事件循环结构体中。 初始化数据库。 启动服务器的事件循环，开始处理外部的指令。 服务器的事件循环EventLoopRedis 把所有的事件注册到一个 aeEventLoop 结构体中，该结构体负责监听是否有事件到来。 redisServer 结构的成员 el 用于事件循环，实现如下： 1234567891011121314typedef struct aeEventLoop { int maxfd; /* highest file descriptor currently registered */ int setsize; /* max number of file descriptors tracked */ long long timeEventNextId; time_t lastTime; /* Used to detect system clock skew */ aeFileEvent *events; /* Registered events */ aeFiredEvent *fired; /* Fired events */ aeTimeEvent *timeEventHead; int stop; void *apidata; /* This is used for polling API specific data */ aeBeforeSleepProc *beforesleep; aeBeforeSleepProc *aftersleep; int flags;} aeEventLoop; server.el 通过函数 aeEventLoop *aeCreateEventLoop(int setsize) 创建，参数 setsize 指定了文件事件列表 aeFileEvent *events 的大小。如果需要销毁 server.el，需要调用函数 void aeDeleteEventLoop(aeEventLoop *eventLoop)。 为了让 Redis 在不同的平台下使用不同的事件处理 api，Redis 把不同平台下的 I/O 多路复用接口封装起来，提供统一的 apiaeApiCreate(eventLoop) 调用。Redis 通过下面的代码确定使用的 I/O 多路复用接口。 12345678910111213#ifdef HAVE_EVPORT#include &quot;ae_evport.c&quot;#else #ifdef HAVE_EPOLL #include &quot;ae_epoll.c&quot; #else #ifdef HAVE_KQUEUE #include &quot;ae_kqueue.c&quot; #else #include &quot;ae_select.c&quot; #endif #endif#endif 例如，Linux 平台下使用 epoll 作为 I/O 多路复用的接口，MacOS 平台使用 kqueue 作为 I/O 多路复用的接口。 server.el 支持两种类型的事件： 文件事件 aeFileEvent 定时器事件 aeTimeEvent 文件事件 文件事件用于处理网络上的操作，包括连接的建立以及连接的读写。在 aeEventLoop 中通过数组保存。访问的时候通过下标 fd 找到对应的文件事件结构体，实现如下： 123456typedef struct aeFileEvent { int mask; /* one of AE_(READABLE|WRITABLE|BARRIER) */ aeFileProc *rfileProc; aeFileProc *wfileProc; void *clientData;} aeFileEvent; Redis 通过以下两个函数注册和删除文件事件。 12int aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData);void aeDeleteFileEvent(aeEventLoop *eventLoop, int fd, int mask); 添加事件的时候需要提供一个回调函数，当事件到达时，需要调用对应的回调函数。 和创建 aeEventLoop 相同，文件事件通过 aeApiAddEvent() 和 aeApiDelEvent() 屏蔽底层实现细节，具体使用的多路复用 API 通过操作系统决定。 定时器事件 定时器事件在 aeEventLoop 中以链表的形式保存，结构实现如下： 123456789101112typedef struct aeTimeEvent { long long id; /* time event identifier. */ long when_sec; /* seconds */ long when_ms; /* milliseconds */ aeTimeProc *timeProc; aeEventFinalizerProc *finalizerProc; void *clientData; struct aeTimeEvent *prev; struct aeTimeEvent *next; int refcount; /* refcount to prevent timer events from being * freed in recursive time event calls. */} aeTimeEvent; Redis 通过以下的函数实现定时器事件的添加和删除。 1234long long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, aeTimeProc *proc, void *clientData, aeEventFinalizerProc *finalizerProc);int aeDeleteTimeEvent(aeEventLoop *eventLoop, long long id); 如果一个定时器事件被触发了，需要调用相应的回调函数 proc()；如果一个定时器事件被取消了，调用相应的回调函数 finalizerProc()。 在服务器初始化的过程中，会注册一个定时器事件 serverCron。根据《Redis 设计与实现》的说法，在 Redis 3.0 仅有这一个定时器事件需要处理，因此采用双向链表实现定时器事件的管理，这样做效率不会下降过多并且保证了实现的简单。 不清楚目前的版本（6.0.9）是否需要处理多个定时器事件，但如果需要呢？Redis 也给出了一种优化思路：采用 skiplist 把插入的事件复杂度降低到 O(log(N)) 事件处理 这部分由 aeMain 负责，就是一个循环，整体的流程如下： 12345678void aeMain(aeEventLoop *eventLoop) { eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) { aeProcessEvents(eventLoop, AE_ALL_EVENTS| AE_CALL_BEFORE_SLEEP| AE_CALL_AFTER_SLEEP); }} 处理事件由函数 aeProcessEvents() 负责，主要执行了以下步骤： 获得最近的定时器事件，事件复杂度为 O(N)。 计算该事件还需要多久才会被触发，计算多路复用的超时时间。 调用多路复用 API，这个操作会阻塞，直到调用超时或者有新的事件触发。 处理文件事件。 处理定时器事件，这一步通过函数 processTimeEvents() 实现。该函数遍历链表，处理每个到期的定时器事件，并重新添加到定时器事件列表中。 参考 《Redis 设计与实现》 Redis 源码阅读 (一) – eventLoop","link":"/2020/12/23/redis-ev/"},{"title":"深信服实习面经","text":"深信服星耀计划 C/C++ 开发（网络安全方向）实习生面经。 一面（约 15min） 写题 输入两个集合 A ，B ，均是由 IP 地址或者连续 IP 地址段组成。其中集合本身 IP 地址段之间是没有交集的，两个集群之间可能有交集。设计合适的数据结构和接口来实现输出 A ，B 两个集合的交集和并集，设计数据结构和对应的 API 实现完成需求，要求占用资源占用最好。 位图 数组表示区间的头尾，排序后使用二分查找。 项目相关 项目流程 项目使用了什么多线程同步手段 自旋锁和互斥锁的区别 APUE 学过没？权限管理是什么？ 二面（约 30min） 一开始耳机有问题，面试官听不到我的声音… 自我介绍 项目相关 项目中遇到了什么困难（并发控制） 使用了什么并发控制手段 条件变量的使用 socket 如何指定使用的网络协议 如何处理大量的连接（I/O 多路复用） select 和 epoll 的区别 还做了什么项目 讲一下数据库比赛的项目 项目中遇到了什么困难 写题 手撕插入排序 优化（二分查找？） 检查代码是否有问题（没问题？） 现在在看什么书，为什么看这本？ 反问 hr 面（约 15min） 自我介绍 为什么选择计算机专业 为什么选择这个岗位 学校社团相关经历 经历的最困难的事情是什么 父母对自己的影响 平时是怎么自学的 反问","link":"/2020/09/06/sangfor/"},{"title":"MIT 6.824 Lab 4（分布式 shard kv）","text":"设计要求 基于之前实现的 Raft 协议（包含 snapshot 机制），实现一个可以容错的分区 Key/Value 数据库。有以下几个具体的要求： 每个 Replica Group 负责存储一部分 K/V 数据，通过 Raft 实现一致性。 Shard Master 负责管理配置信息，决定如何分片，通过 Raft 实现一致性。 客户端通过 Shard Master 得知所有的分片信息，Replica Group 定期拉取最新的配置。 支持分片在不同的 Replica Group 中移动，应对 Replica Group 下线或者重新上线的情况。 在变更配置的过程中，如果一个 key 不需要迁移，可以继续提供对外服务。 变更配置后，原有的节点不需要保存过期的数据。 整体架构 整体结构是一个典型的 Master/Worker 结构。实现的功能比较基础，有些功能没有实现，比如，Raft Group 的结点不能变更，Client 不允许并发操作。 Shard Master 实现了以下的功能： 存储配置信息，配置信息包含了 Replica Group 服务器的地址，以及每个 Replica Group 具体负责处理哪些 Shard。 分配 Shard 给不同的 Replica Group 处理，每个 Replica Group 分配的 Shard 数量要尽可能平均。 响应 Join/Leave/Move/QueryRPC 请求，并且处理配置的变更。 Replica Group 实现了以下的功能： 存储对应的 Shard，响应客户端对于 Shard 的 Get/Put/Append 请求。 周期性向 Shard Master 询问配置信息是否变更，如果有变更及时完成数据的迁移。 在数据分片迁移的过程中，不能响应客户端对于当前数据分片的请求，直到分片迁移完成。 Shard Master 实现 RPC 请求（Join/Leave/Move/Query）的实现 主要分为客户端和服务端两个方面。 客户端： 每个客户端分配一个指定的序号，同时每个请求分配一个递增的请求序号，保证所有的操作只执行一次。 每个客户端同时只能有一个请求等待响应，不能同时发送多个请求。 服务端： 服务端需要一个单独的线程执 apply 操作，接收所有通过 Raft 达成共识的消息。 在接收到消息的时候，需要执行去重判断（根据客户端的请求序号），保存一个 map 存储每个客户端最大的请求序号，只有请求序号超过当前客户端最大的请求序号的请求才能执行。 客户端需要等到请求达成共识后才能返回。 服务端如何才能判断客户端的请求达成共识？ 我们可以根据 Raft 协议的安全性实现。Raft 需要满足 State Machine Safety，定义如下： State Machine Safety: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. 这个特性说明如果在某个 index 上的消息达成了共识，那么这个 index 上的消息就不会改变。 因此，Client 在写入 Command 并获得写入的 index 后，创建对应 index 的 channel 并监听，如果 channel 返回的命令和发送的命令相等，表示命令执行成功。 Reconfigure当 Shard Master 接收到 Join/Leave 消息时，需要对于分片数据进行重新划分。有以下几个划分思路： 一致性哈希。把整个数据分片作为一个哈希环，每一个 Replica Group 计算一个哈希值并映射到哈希环上，按照一个顺序（顺时针 / 逆时针）把哈希环上的每个分区的数据放到对应的下一个 Replica Group 中。这个做法不能保证 Shard 的平均分配。 范围分片。为每个 Relica Group 划分一个范围，负责这个范围内所有数据的读取写入操作。这个做法可以保证分区需要处理的数量是基本平衡的。 当有 Replica Group 加入或者退出，该怎么处理？ 最简单的方法是直接重新分片。计算每个分片需要处理多少个分区数据，然后依次遍历每个 Replica Group 并进行分配。目前采用这个版本实现分片重新配置的算法。这个方法的问题在于可能会导致大量的数据在网络上传输，因为这样做会导致原本不需要迁移的数据因为换了一个 Replica Group 而需要迁移，造成了网络带宽的浪费。 另一种方法的出发点是只传输必要的数据量。首先计算出每个 Replica Group 需要存储的最小数据分片数量（因为这是需要传输数据量的最小值）。如果是加入操作，那么挑选当前数据分片最多的 Replica Group，每次迁出一个 Shard，直到传输数量和计算的数量相等。迁出操作和加入操作相反，挑选当前数据分片最少的 Replica Group，每次迁入一个 Shard。 Replica Group 实现 KV 数据的存储方式 由于设计需要支持迁移，并且迁移完成后删除不需要的数据，把服务器上所有的数据保存在同一个 map 中不是一个合适的行为，这样在遍历过程中需要访问所有数据，效率不高。因此，数据存储的方式应该是分别存放每一个 Shard 的数据。Shard 整体的结构见下。 12345type Shard struct { ID int DB map[string]string LastSerial map[int64]int} DB 保存了具体的数据，LastSerial 存储了每个 Client 对应的最大 RPC 请求编号。用于保证一致性的实现。 感知配置的变化 Add code to periodically fetch the latest configuration from the shardmaster, and add code to reject client requests if the receiving group isn’t responsible for the client’s key’s shard. 因此，为了感知配置的变化，Replica Group 的 leader 需要定期向 Shard Master 询问当前的配置是否过期，如果过期了，leader 需要给所有的 follower 发送一条消息，告知配置过期，需要更新配置。 更新配置的步骤如下： 如果一个分区当前属于这个节点，但是下一个配置需要转移出去，那么这个分区就暂时不能提供服务，并把它缓存到一个队列中。 所有过期数据全部收集完毕后，开始分区转移流程。 分区转移 假设分区 S 的旧 Replica Group 为 G1，新 Replica Group 为 G2。 实现分区的迁移有以下两种思路： G2 向 G1 拉取分区 S。 G1 向 G2 推送分区 S。 这两种方法都是可行的，考虑到需要实现删除过期数据的要求，我的实现采用了第二种方法，也就是推送的方法，这种方法在对方接收到发送的 Shard 之后就可以删除不需要的 Shard，而第一种方法则需要多发送一次 RPC。 分区转移的发送方流程如下： 根据配置信息，获取每一个在缓存队列的 Shard 的目的地址。 把 Shard 封装为分区转移请求并发送过去，等待目的 Replica Group 把发送的 Shard 接收。 接收完成后，删除对应的缓存 Shard。 为了实现上述的过程，用于迁移的缓存数据结构设计如下： 123456type Migrate struct { Shard int Config int DB map[string]string LastSerial map[int64]int} 其中，Config 字段表示这个数据属于哪个 configuration。 分区转移的接收方需要判断接收到的 Shard 是否需要自己处理。如果有的 Replica Group 感知配置变更的速度较慢，或者网络乱序，接收方都有可能收到过期的分区转移请求。接收方接收到一个分区后，执行以下的处理操作： 无论如何，接收到的 Shard 在某个配置中自己负责的，因此需要通过 Raft 告知 Replica Group 中的 Follower 接收到了新的分区数据，并且通知发送方删除。 如果当前分区是自己在当前配置需要处理的，那么直接保存，并且让该分区可以对外提供服务。 如果当前分区不是自己在当前配置需要处理的，缓存到缓存队列。，找到它在接下来的配置中的目的地址，构造分区转移请求，执行分区转移的发送方法。 这个方法要求每个 Replica Group 都需要知道所有的配置变更信息。因此，感知配置变更的时候，每个 Replica Group 查询的是是否存在当前配置的下一个配置，而不是查找最新的配置。 如果不这样做，可能会出现以下情况： 假设分区 S 的旧 Replica Group 为 G1，新 Replica Group 为 G2。 发生配置变更，G1 发送 S 给 G2，G2 的响应丢失，G1 没能成功删除 S。 G2 响应了对 S 的写入请求。 配置再次发生变更，新 Replica Group 为 G3，此时 G1，G2 都发送 S 给 G3，可能会出现 G1 直接更新到 G3 的情况。 不查询最新的配置可以避免出现上述情况。","link":"/2020/12/12/shardkv/"},{"title":"《Redis 设计与实现》读书笔记","text":"本文记录了《Redis 设计与实现》中比较重要的内容。 数据结构 字典 字典在 Redis 中应用广泛。例如，字典键的实现、数据库的底层实现。 Redis 采用 MurmurHash2 算法计算哈希键值，采用链地址法解决哈希冲突。 当 Redis 的负载因子达到一定的大小，会采用 rehash 操作。执行 rehash 的时机如下： 当前数据库没有执行 BGSAVE 或 SAVE 指令，并且负载因子大于 1。 当前数据库正在执行 BGSAVE 或 SAVE 指令，并且负载因子大于 5。 这样做的目的是减少写时拷贝过程中内存的修改大小，提升性能。 由于一次性对大量的键值对（千万级或者亿级）执行 rehash 操作会阻塞当前服务器线程，无法继续执行命令，Redis 采用了渐进式 rehash 方法完成 rehash 操作。具体步骤如下： 为 ht[1] 分配空间，让字典同时拥有 ht[0] 和 ht[1]。 初始化索引计数器 rehashidx 为 0，表示 rehash 正式开始。 每当执行查找、插入、删除操作时，顺带对 ht[0][rehashidx] 的所有元素执行 rehash 操作到 ht[1] 中，完成后，rehashidx++。 当所有的键值对迁移完毕后，回收 ht[0] 的内存空间，把 ht[1] 变为 ht[0]，同时新建一个 ht[1] 用于下一次的 rehash 过程。 在执行 rehash 的过程中，对字典的删除、查找、更新操作会同时在两个哈希表上进行。例如，要在字典中查找一个键，程序首先在 ht[0] 中查找，如果没找到则在 ht[1] 中查找。对字典的插入操作直接在 ht[1] 上执行。 跳跃表 跳跃表是一种有序的数据结构，性能和平衡树相差不大，并且实现简单。 Redis 使用 zskiplistNode 结构表示一个跳跃表节点，有以下几个成员： 层：层高是一个 1-32 的随机数。包含前进指针和跨度两个属性。前进指针指向当前层的下一个节点，跨度记录了两者之间的距离，用于计算目标节点排位。 后退指针：用于表尾到表头的遍历。 分值：每个节点按照分值由小到大排序。如果分值相同，按照对象大小排序。 成员：一个字符串对象。 为了便于处理整个跳跃表，Redis 使用 zskiplist 管理整个跳跃表，这个结构包含了指向头尾节点的指针 head 和 tail，通过这两个指针，程序访问头尾节点的时间复杂度是 O(1)。通过 length 保存整个跳跃表的长度，获取长度的时间复杂度是 O(1)。同理，通过 level 保存跳跃表的最大层级。 整数集合 当 SET 内容不多，并且都是整数时，Redis 使用整数集合作为底层实现。 整数集合的元素类型包括 int16_t, int32_t, int64_t。所有的数据按照顺序由小到大排列。当新插入的元素的长度超过了整数集合中所有元素的长度，那么需要对整数集合执行升级操作。这种做法保证了灵活性，同时节约了内存。 整数集合的升级包括了以下的步骤： 根据新元素的类型，分配合适的底层数组空间大小。 将原有的元素转换为和新元素相同的类型，并将类型转换后的数放置在正确的位置上，保证相对顺序不变。 将新元素放到指定的位置。 整数集合不支持降级操作。 压缩链表 压缩列表是哈希键和集合键的底层实现之一，当数据量不大，并且列表键的列表项或者哈希键的键值是整型或者短字符串，就会使用压缩链表作为底层实现。 压缩链表的节点有以下几个字段构成：previous_entry_length、encoding、content。 previous_entry_length：前一个字段的长度。如果小于 254 字节，那么这个字段只有一个字节；否则，该字段有五个字节，第一个字节的内容是 254，后面的字节存储长度。 encoding：内容的编码方式。 content：具体内容。 压缩链表存在的问题是连锁更新：插入新的内容导致前面 entry 的长度变化，会导致后面的 entry 需要更多的内存保存 previous_entry_length，造成后续字段长度都发生了变化。这样会造成内存的多次重新分配。 对象Redis 一共有五种对象，分别为：字符串对象、列表对象、哈希对象、集合对象、有序集合对象。每种对象在不同的使用场景下有不同的底层编码方式，可以优化使用效率。 STRING 对象编码方式分为三种：int、embstr 和 raw。embstr 针对短字符串进行了优化，减少了内存分配和释放的次数，并且由于采用连续存储的方式，提升了缓存的命中率。 LIST 对象编码方式分为两种：ziplist 和 linkedlist。当所有项的大小小于 64 字节，并且列表长度小于 512，使用 ziplist 编码。 HASH 对象编码方式有两种：ziplist 和 hashtable。当键值对的所有键和值的大小小于 64 字节，并且对象长度小于 512，使用 ziplist 编码方式。 SET 对象编码方式有两种：intset 和 hashtable。当所有的值都是整数，并且对象长度小于 512，使用 intset 作为底层编码方式。 ZSET 对象编码方式有两种：ziplist 和 skiplist。当所有元素的长度都小于 64 字节，同时长度小于 128 时，使用 ziplist 作为底层编码方式。 skilplist 编码方式同时使用了哈希表和跳跃表，兼顾了精确查找和范围查找的效率。 Redis 通过引用计数的方式实现内存的自动回收。同时，通过引用计数也可以实现对象的复用。 单机数据库 键过期删除策略Redis 把所有键的过期时间添加到一个过期字典中，检查过期字典就可以判断一个键是否过期。 过期删除策略分为三种：定时删除、惰性删除、定期删除。 定时删除创建一个定时器，当定时器过期时，立刻删除数据键。这种策略对内存最为友好，但是会对 CPU 造成更多的负担。 惰性删除每次访问键时检查键是否过期，如果过期就删除这个键。这种策略对 CPU 最为友好，但是会造成内存的浪费。 定期删除结合了二者的优点。 Redis 采用惰性删除和定期删除两种过期删除策略。 RDB 持久化RDB 持久化通过保存当前 Redis 数据库状态实现持久化。RDB 有两种命令可以实现持久化操作：SAVE 和 BGSAVE。 SAVE 操作阻塞当前进程，直到持久化操作完成，服务器不会执行新的命令。 BGSAVE 操作创建一个子进程执行持久化操作，在这个期间，服务器依旧可以执客户端发送的命令。 写入 RDB 文件的过程中，服务器会检查每个键是否是过期的，只写入没有过期的键。在载入 RDB 文件的过程中，如果服务器是在主模式下运行，那么会检查键是否过期；否则会把 RDB 文件全部载入。 可以对数据库设置 RDB 文件的自动保存条件。Redis 通过 dirty 计数器保存上一次执行 RDB 操作后数据库的修改次数，通过 lastsave 时间戳记录距离上一次 RDB 操作的事件。每 100ms 就会检查这两个值，如果达到了自动保存条件就会执行 RDB 持久化操作。 RDB 文件结构如下 每个数据库结构如下 不带过期时间的键值对结构如下 带过期时间的键值对结构如下 AOF 持久化AOF 持久化通过保存 Redis 服务器的写命令实现保存数据库的状态。 为防止 AOF 文件过大，服务器定期执行 AOF 重写操作，使用更少的指令达到相同的数据库状态，减少冗余。 可以通过设置 appendfsync 选项的值控制 AOF 文件的刷新行为。 appendfsync 选项的值 AOF 刷新行为 always 将 aof_buf 缓冲区的所有内容写入，并同步到 AOF 文件中。 everysec 将 aof_buf 缓冲区的所有内容写入到 AOF 文件中，如果距离上次同步 AOF 文件的时间超过一秒，那么再次对 AOF 文件进行同步，并且同步过程由专门的进程负责。 no 将 aof_buf 缓冲区的所有内容写入，不对 AOF 文件同步，具体的同步时间由操作系统决定。 appendfsync 选项的值直接决定了 AOF 持久化功能的效率和安全性。 如果 appendfsync 设置为always，写入的速度是最慢的，但是安全性是最高的，出现故障时仅仅丢失一个事件循环中写入的数据。 如果 appendfsync 设置为everysec，这个模式足够快，并且出现故障时丢失的时最近一秒内写入的数据。 如果 appendfsync 设置为no，写入速度是最快的，但是每次同步需要消耗的时间是最多的。 为了避免 AOF 文件过大造成数据库恢复时间过长的问题，Redis 提供了 AOF 重写的功能。重写功能采用读取当前数据库的状态实现的。重写操作会执行大量的写入操作，调用的进程会陷入长时间的阻塞状态，因此 Redis 使用一个单独的子进程执行 AOF 重写操作。 在子进程重写的过程中，服务器可能会接收到新的写命令，造成 AOF 文件状态与当前数据库状态的不一致。为了解决这个问题，Redis 设置了一个 AOF 缓冲区。在 AOF 重写过程中，服务器将接收到的写命令保存到缓冲区中。当子进程的 AOF 重写操作完成后，把缓冲区的内容写入到新的 AOF 文件中。 事件Redis 是一个事件驱动程序，事件类型分为文件事件和时间事件。 文件事件处理器基于 Reactor 模式开发，通过包装 I/O 多路复用函数实现。有两种类型的事件： 可读事件：客户端执行 write 操作、客户端执行 close 操作、客户端执行 connect 操作； 可写事件：客户端执行 read 操作。 时间事件有以下两种类型： 定时事件：让一段程序在指定的时间后执行一次； 周期性事件：让一段程序每隔一段时间执行一次。 所有时间事件存储在一个无序链表中，执行时遍历整个链表，处理所有已到达的时间事件。 原因：Redis 服务器只有一个周期性事件，使用链表不会影响性能。 客户端Redis 使用一个链表保存所有建立连接的客户端，客户端结构体保存了客户端相关的属性。 输入缓冲区保存客户端输入的命令。这个缓冲区是可以动态扩展的，但是最大大小不能超过 1GB。 输出缓冲区保存服务端的响应，由固定大小缓冲区和可变大小缓冲区组成。 服务器 服务器接收到客户端发送的命令后，执行以下几个步骤： 读取命令请求，缓存在客户端结构体中； 执行命令，包含了以下四步： 查找命令实现 执行预备工作 调用命令的执行函数 执行后续工作。 将命令回复发送给客户端。 在执行预备工作的阶段，如果打开了 maxmemory 功能，那么先检查服务器的内存占用情况，必要时会执行内存回收的操作。 服务器每 100 毫秒执行一次 serverCron 函数。该函数主要的工作包括：更新服务器缓存、更新 LRU 时钟、检查持久化条件、执行被延迟的 BGREWRITEAOF 操作。 多机数据库 复制 复制分为两个步骤：同步和命令传播。 命令传播：主服务器会将写命令发送给从服务器。 同步：同步分为完全重同步和部分重同步： 完全重同步：适用于首次进行主从同步的情况。主服务器执行 BGSAVE 生成 RDB 文件，同时记录所有的写命令。把这些内容发送给从服务器，即可完成同步。 部分重同步：用于断线重连后主从复制，如果条件允许，主服务器将在连接断开后接收到的写命令发送给从服务器，从而解决了断线重连后完全重同步的低效问题。 部分重同步的实现依赖于三个机制：复制偏移量、复制积压缓冲区、服务器的运行 id。 复制偏移量用于确定主从服务器之间是否处于一致状态。如果主从服务器的复制偏移量相同，那么他们就处于一致状态；反之，他们就处于不一致状态。 复制积压缓冲区保存主服务器在命令传输阶段发送的写命令。当从服务器发送 PSYNC 命令时，如果复制积压缓冲区的内容包含所有缺失的数据，那么执行部分重同步操作；反之，执行完全重同步操作。 服务器的运行 id 用于确定从服务器复制的主服务器。当从服务保存的运行 id 和主服务器的运行 id 相同，那么主服务器可以尝试执行部分重同步操作；反之，主服务器需要使用完全重同步操作。 psync()的流程见下图。 从服务器通过心跳机制检测检查网络连接状态以及检测命令丢失情况。 Sentinel（哨兵机制）由一个或者多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器，以及这些服务器下属的从服务器。如果被监视的主服务器下线，Sentinel 系统可以从下属的从服务器中选出一个服务器作为新的主服务器。Sentinel 保证了 Redis 系统的高可用性。 Sentinel 和每个服务器（主服务器和从服务器）建立了一条命令连接和一条订阅连接。完成后，Sentinel 每 10 秒发送一次 INFO 命令获取主服务器的当前信息，用于更新主服务器实例。在这个过程中，如果 Sentinel 系统发现了新的从服务器，会和这个从服务器之间建立一条命令连接和一条订阅连接，用于更新从服务器实例。 Sentinel 每 2 秒一次向主服务器和从服务器发送频道信息，并通过订阅连接接收频道消息，用于更新其他监视该服务器的 Sentinel 信息。当 Sentinel 发现了新的 Sentinel 后，会更新 sentinels 字典，并和该 Sentinel 建立一条命令连接（没有订阅连接）。 Sentinel 每 1 秒发送一次 PING 指令，持续回复无效命令超过一个固定的时间，Sentinel 判断主服务器已经下线（主观下线）。随后 Sentinel 会询问其他的 Sentinel 该服务器是否下线，如果足够多的 Sentinel 判断服务器已经处于主观下线状态，该服务器进入客观下线状态。 当一个主服务器处于客观下线状态后，监视该主服务器的 Sentinel 需要选举出一个领头 Sentinel，执行故障转移操作。选举流程采用 Raft 算法的一个实现，具体流程如下： 所有的 Sentinel 都有被选举为领头 Sentinel 的资格。 每个发现自己监听的主服务器处于客观下线状态的 Sentinel 都会要求其他的 Sentinel 设置自己为局部领头 Sentinel。 最先向目标 Sentinel 发送设置要求的源 Sentinel 会被设置为目标 Sentinel 的局部领头 Sentinel，其他的设置要求都会被忽略。（即先到先得） 如果某个 Sentinel 被半数以上的 Sentinel 选举为局部领头 Sentinel，这个 Sentinel 将成为领头 Sentinel。 如果在规定时间内没有选出领头 Sentinel，那么过一段时间后再次进行选举，直到选出领头 Sentinel 为止。 领头 Sentinel 负责故障转移操作，包括确定新的主服务器、修改从服务器的复制目标、将下线的主服务器设置为新主服务器的从服务器。 选举新主服务器步骤如下： 删除已经下线的从服务器（保证从服务器都是在线的）。 删除最近 5 秒内没有回复过领头 Sentinel 的 INFO 命令的从服务器（保证从服务器的网络状态良好）。 删除和主服务器断开连接超过 down-after-milliseconds*10 的从服务器（保证从服务器的数据内容都是尽可能新的）。 之后，领头 Sentinel 根据服务器的优先级排序，选出优先级最大的从服务器。如果有多个最高优先级的从服务器，选择复制偏移量最大的从服务器。如果有多个优先级最高、复制偏移量最大的从服务器，选择 ID 最小的从服务器。 集群 节点通过 CLUSTER MEET 命令连接。当节点 A 和节点 B 完成握手后，节点 A 通过 Gossip 协议扩散给所在集群的其他节点，告知节点 B 已经加入到集群中了。 Gossip 协议的执行过程如下：Gossip 过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，直至最终网络中所有的节点都收到了消息。这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个最终一致性协议。 Redis 集群采用分片的方式保存键值对，数据库总共分为 16384 个槽，每个 key 都属于其中一个槽，每个服务器可以同时处理 0-16384 个槽中的键。 每个槽的指派信息记录在一个 bitmap 中，同时记录了集群中所有槽指派信息。在集群中，每个节点通过消息传播的方式，告知集群内其他的节点自己负责的槽。 当客户端向集群内的任意节点发送和键相关的命令时，节点需要计算出键属于哪个槽，并且判断这个槽是否由自己负责的。如果不是自己负责的，需要给客户端发送一个 MOVE 错误。客户端接收到这个消息后，将会被重定向到正确的节点，然后再次发送相同的命令。 Redis 使用 CRC-16 校验和确定每个数据键属于哪个槽。 Redis 集群提供了重新分片的功能。重新分片指的是将任意数量的指派给某个节点（源节点）的槽划分给另一个节点（目标节点），Redis 使用管理软件 redis-trib 执行。重新分片操作可以在线进行，集群不需要下线。在操作过程中，如果客户端向源节点发送数据键相关的命令，并且数据键属于正在被迁移的槽，那么源节点首先在自己的数据库查找键，如果不存在，向客户端发送一个 ASK 错误。客户端接收到这个错误后，将会重定向到目标节点，并且重新执行数据键相关的命令。 MOVE 错误和 ASK 错误的区别如下： MOVE 命令的效果是永久性的。 ASK 命令的效果是临时性的。 集群内每个节点通过发送 PING 消息确定集群内的节点是否在线。如果接收了 PING 消息的节点没有在规定时间内返回 PONG 消息，那么发送 PING 消息的节点会把接收 PING 消息的节点标记为 ** 疑似下线 ** 状态。如果有半数以上的主节点认为某个主节点处于疑似下线状态，那么该节点会被标记为已下线，并通过消息广播的形式通知集群内所有的节点。 当 Redis 集群中的主节点下线后，需要选出一个从节点成为新的主节点。选举的过程和 Sentinel 选举领头节点的过程类似，也是 Raft 算法的实现。选举过程如下： 当从节点发现自己的主节点已下线，它会要求所有具有投票权的主节点给自己投票。 主节点会把接收到的第一个从节点设置为新的主节点。 如果一个从节点获得 n/2+1 个主节点的投票支持，那么就会成为新的主节点。 参考 《Redis 设计与实现》 P2P 网络核心技术：Gossip 协议","link":"/2020/09/09/redis-notes/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"epoll","slug":"epoll","link":"/tags/epoll/"},{"name":"MIT 6.824","slug":"MIT-6-824","link":"/tags/MIT-6-824/"},{"name":"系统设计","slug":"系统设计","link":"/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"name":"Raft","slug":"Raft","link":"/tags/Raft/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"源码","slug":"源码","link":"/tags/%E6%BA%90%E7%A0%81/"},{"name":"深信服","slug":"深信服","link":"/tags/%E6%B7%B1%E4%BF%A1%E6%9C%8D/"},{"name":"面经","slug":"面经","link":"/tags/%E9%9D%A2%E7%BB%8F/"},{"name":"技术书籍","slug":"技术书籍","link":"/tags/%E6%8A%80%E6%9C%AF%E4%B9%A6%E7%B1%8D/"}],"categories":[{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"网络编程","slug":"网络编程","link":"/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"C","slug":"C","link":"/categories/C/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"求职","slug":"求职","link":"/categories/%E6%B1%82%E8%81%8C/"}]}